{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b570978a",
   "metadata": {},
   "source": [
    "# Requirements:\n",
    "1. Introduction about the dataset\n",
    "2. What is the goal\n",
    "3. Preprocessing text\n",
    "4. Visualization\n",
    "5. Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dde4f2",
   "metadata": {},
   "source": [
    "# Rubric:\n",
    "1. Definition of the dataset - Comprehensive and clear definition\n",
    "2. Visualization of the Dataset Features and Results - Detailed and insightful visualizations\n",
    "3. Cleaning the Text Document with Two Approaches - Effective use of two distinct cleaning approaches, with clear explanations\n",
    "4. Implementation of Three Word Embedding Methods - Comprehensive and effective implementation of three methods, including one\n",
    "with SpaCy\n",
    "5. Comparison of the Results of Word Embedding Methods - In-depth and insightful comparison, with clear findings\n",
    "6. Report and Description of Work Done - Comprehensive and well-structured report, clearly describing all aspects of the\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be97e59",
   "metadata": {},
   "source": [
    "# Introduction about the dataset\n",
    "\n",
    "The dataset used in this project is a labeled dataset of fake news articles, collected from various sources. The dataset contains 21261 instances, with each instance containing a title, text, and label (true or fake). The labels are represented as 0 for fake news and 1 for true news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c784f1",
   "metadata": {},
   "source": [
    "# What is the goal\n",
    "\n",
    "The goal of this project is to develop a machine learning model that can accurately classify fake news articles based on their content. By leveraging various word embedding techniques, such as Word2Vec, GloVe, and FastText, we will explore the potential of these models in detecting and distinguishing fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b75420",
   "metadata": {},
   "source": [
    "# Preprocessing text\n",
    "\n",
    "## Loading dataset from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47885a94-be55-4944-8e39-93571036b752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled!\n",
      "Load Data                        :       0.54 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from gensim.models import Word2Vec, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Enable GPU if available\n",
    "if spacy.require_gpu():\n",
    "    print(\"GPU is enabled!\")\n",
    "else:\n",
    "    print(\"GPU not available.\")\n",
    "\n",
    "# Decorator to measure time of the function\n",
    "def measure_time(description):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "            print(f\"{description:<33}: {end_time - start_time:>10.2f} seconds\")\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Load the dataset\n",
    "@measure_time(\"Load Data\")\n",
    "def load_data():\n",
    "    train_ds = pd.read_csv(\"Datasets/Fakenews/train.csv\", sep=';', on_bad_lines='skip') \n",
    "    test_ds = pd.read_csv(\"Datasets/Fakenews/test.csv\", sep=';', on_bad_lines='skip') \n",
    "    return train_ds, test_ds\n",
    "\n",
    "train_ds, test_ds = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84991c41",
   "metadata": {},
   "source": [
    "## Cleaning Text using 2 approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup pipeline for cleaning text\n",
    "@Language.component(\"custom_cleaner\")\n",
    "def custom_cleaner(doc):\n",
    "    # Process multiple tokens at once\n",
    "    tokens = [token.lemma_ or token.text for token in doc \n",
    "             if  not token.is_punct\n",
    "             and not token.is_stop\n",
    "             and not token.is_space\n",
    "             and not token.is_digit\n",
    "             and not token.is_currency\n",
    "             and not token.is_quote\n",
    "             and not token.is_bracket]\n",
    "    return Doc(doc.vocab, words=tokens)\n",
    "\n",
    "def setup_pipeline():\n",
    "    nlp1 = spacy.load('en_core_web_sm', disable=['tok2vec','ner', 'parser', 'lemmatizer'])\n",
    "    nlp2 = spacy.load('en_core_web_sm', disable=['tok2vec','ner', 'parser'])\n",
    "    nlp2.add_pipe(\"custom_cleaner\", last=True)\n",
    "    return nlp1, nlp2\n",
    "\n",
    "\n",
    "nlp1, nlp2 = setup_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3fdd3",
   "metadata": {},
   "source": [
    "## Word Embedding using 3 methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42a1fc",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9aa01f",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb2eb9",
   "metadata": {},
   "source": [
    "## Reference(s):\n",
    "Singh, A. (n.d.). Fake News Classification. Kaggle. Retrieved January 17, 2025, from https://www.kaggle.com/datasets/aadyasingh55/fake-news-classification/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e258a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled!\n",
      "Load Data                        :       0.51 seconds\n",
      "Prepare Data                     :     309.49 seconds\n",
      "Embedding Word2Vec               :      77.45 seconds\n",
      "Embedding BOW                    :       5.43 seconds\n",
      "Embedding GloVe                  :     244.75 seconds\n",
      "Train Model Word2Vec             :       1.04 seconds\n",
      "Train Model BoW                  :      97.13 seconds\n",
      "Train Model GloVe                :       1.29 seconds\n",
      "Model Pipeline1 Word2Vec Accuracy:     0.9761\n",
      "Model Pipeline2 Word2Vec Accuracy:     0.9714\n",
      "Model Pipeline1 BOW      Accuracy:     0.9786\n",
      "Model Pipeline2 BOW      Accuracy:     0.9816\n",
      "Model Pipeline1 GloVe    Accuracy:     0.9681\n",
      "Model Pipeline2 GloVe    Accuracy:     0.9605\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from gensim.models import Word2Vec, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "@Language.component(\"custom_cleaner\")\n",
    "def custom_cleaner(doc):\n",
    "    # Process multiple tokens at once\n",
    "    tokens = [token.lemma_ or token.text for token in doc \n",
    "             if  not token.is_punct\n",
    "             and not token.is_stop\n",
    "             and not token.is_space\n",
    "             and not token.is_digit\n",
    "             and not token.is_currency\n",
    "             and not token.is_quote\n",
    "             and not token.is_bracket]\n",
    "    return Doc(doc.vocab, words=tokens)\n",
    "\n",
    "def setup_pipeline():\n",
    "    nlp1 = spacy.load('en_core_web_sm', disable=['tok2vec','ner', 'parser', 'lemmatizer'])\n",
    "    nlp2 = spacy.load('en_core_web_sm', disable=['tok2vec','ner', 'parser'])\n",
    "    nlp2.add_pipe(\"custom_cleaner\", last=True)\n",
    "    return nlp1, nlp2\n",
    "\n",
    "def process_batch(texts, nlp, batch_size=1000):\n",
    "    return list(nlp.pipe(texts, batch_size=batch_size))\n",
    "\n",
    "def pipeline_1(texts, nlp): \n",
    "    # Process in batches\n",
    "    docs = process_batch([text.lower() for text in texts], nlp)\n",
    "    return [' '.join([token.text for token in doc]) for doc in docs]\n",
    "\n",
    "def pipeline_2(texts, nlp): \n",
    "    # Process in batches\n",
    "    docs = process_batch([text.lower() for text in texts], nlp) \n",
    "    return [' '.join([token.text for token in doc]) for doc in docs]\n",
    "    \n",
    "# Handle out of word errors for Word2Vec model testing.\n",
    "def get_word_vector(word, model, vector_size=500):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "def get_doc_vector(tokens, model):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vectors.append(model.wv[token])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    return np.zeros(model.vector_size)\n",
    "\n",
    "\n",
    "\n",
    "@measure_time(\"Prepare Data\")\n",
    "def prepare_data(train_ds, test_ds, nlp1, nlp2):\n",
    "    # Prepare Train data - Pipeline 1 and 2\n",
    "    train_ds['pipeline_1'] = pipeline_1(train_ds['text'].tolist(), nlp1)\n",
    "    train_ds['pipeline_2'] = pipeline_2(train_ds['text'].tolist(), nlp2)\n",
    "\n",
    "    # Prepare Test data - Pipeline 1 and 2\n",
    "    test_ds['pipeline_1'] = pipeline_1(test_ds['text'].tolist(), nlp1)\n",
    "    test_ds['pipeline_2'] = pipeline_2(test_ds['text'].tolist(), nlp2)\n",
    "    \n",
    "    # Prepare result dataset\n",
    "    train_y = train_ds['label'].values\n",
    "    test_y = test_ds['label'].values\n",
    "\n",
    "    return train_ds, test_ds, train_y, test_y\n",
    "\n",
    "@measure_time(\"Embedding Word2Vec\")\n",
    "def embedding_word2vec(train_ds, test_ds):\n",
    "    word2vec_model1 = Word2Vec(train_ds['pipeline_1'].apply(lambda x: x.split()), vector_size=500, window=5, min_count=1, workers=4)\n",
    "    word2vec_model2 = Word2Vec(train_ds['pipeline_2'].apply(lambda x: x.split()), vector_size=500, window=5, min_count=1, workers=4)\n",
    "\n",
    "    train_a1_X1 = [get_doc_vector(text.split(), word2vec_model1) for text in train_ds['pipeline_1']]\n",
    "    train_a1_X2 = [get_doc_vector(text.split(), word2vec_model2) for text in train_ds['pipeline_2']]\n",
    "\n",
    "    test_a1_X1 = [np.mean([get_word_vector(word, word2vec_model1) for word in text.split()], axis=0) for text in test_ds['pipeline_1']]\n",
    "    test_a1_X2 = [np.mean([get_word_vector(word, word2vec_model2) for word in text.split()], axis=0) for text in test_ds['pipeline_2']]\n",
    "\n",
    "    return train_a1_X1, train_a1_X2, test_a1_X1, test_a1_X2\n",
    "\n",
    "@measure_time(\"Embedding BOW\")\n",
    "def embedding_bow(train_ds, test_ds):\n",
    "    vectorizer1 = CountVectorizer(max_features=10000)\n",
    "    vectorizer2 = CountVectorizer(max_features=10000)\n",
    "\n",
    "    train_a2_X1 = vectorizer1.fit_transform(train_ds['pipeline_1']).toarray()\n",
    "    train_a2_X2 = vectorizer2.fit_transform(train_ds['pipeline_2']).toarray()\n",
    "\n",
    "    test_a2_X1 = vectorizer1.transform(test_ds['pipeline_1']).toarray()\n",
    "    test_a2_X2 = vectorizer2.transform(test_ds['pipeline_2']).toarray()\n",
    "\n",
    "    return train_a2_X1, train_a2_X2, test_a2_X1, test_a2_X2\n",
    "\n",
    "@measure_time(\"Embedding GloVe\")\n",
    "def embedding_glove(train_ds, test_ds):\n",
    "    glove_model = spacy.load('en_core_web_lg') \n",
    "    glove_model.select_pipes(enable=['tok2vec'])\n",
    "    \n",
    "    vector_size = glove_model.vocab.vectors.shape[1]  # Get vector dimension    \n",
    "    def get_doc_vector(doc):\n",
    "        # Handle string input\n",
    "        if isinstance(doc, str):\n",
    "            doc = glove_model(doc)\n",
    "            \n",
    "        # Get vectors for tokens that have them\n",
    "        vectors = [token.vector for token in doc if token.has_vector]\n",
    "        \n",
    "        # Return mean of vectors or zeros if no vectors\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    train_a3_X1 = [get_doc_vector(text) for text in train_ds['pipeline_1']]\n",
    "    train_a3_X2 = [get_doc_vector(text) for text in train_ds['pipeline_2']]\n",
    "\n",
    "    test_a3_X1 = [get_doc_vector(text) for text in test_ds['pipeline_1']]\n",
    "    test_a3_X2 = [get_doc_vector(text) for text in test_ds['pipeline_2']]\n",
    "\n",
    "    return train_a3_X1, train_a3_X2, test_a3_X1, test_a3_X2\n",
    "\n",
    "@measure_time(\"Train Model Word2Vec\")\n",
    "def train_model_word2vec(train_a1_X1, train_a1_X2, train_y):\n",
    "    model_p1_a1 = LogisticRegression(max_iter=2000)\n",
    "    model_p1_a1.fit(train_a1_X1, train_y)\n",
    "\n",
    "    model_p2_a1 = LogisticRegression(max_iter=2000)\n",
    "    model_p2_a1.fit(train_a1_X2, train_y) \n",
    "  \n",
    "    return model_p1_a1, model_p2_a1\n",
    "\n",
    "@measure_time(\"Train Model BoW\")\n",
    "def train_model_bow(train_a2_X1, train_a2_X2, train_y):\n",
    "    model_p1_a2 = LogisticRegression(max_iter=2000)\n",
    "    model_p1_a2.fit(train_a2_X1, train_y)\n",
    "\n",
    "    model_p2_a2 = LogisticRegression(max_iter=2000)\n",
    "    model_p2_a2.fit(train_a2_X2, train_y) \n",
    "  \n",
    "    return model_p1_a2, model_p2_a2\n",
    "\n",
    "@measure_time(\"Train Model GloVe\")\n",
    "def train_model_glove(train_a3_X1, train_a3_X2, train_y):\n",
    "    model_p1_a3 = LogisticRegression(max_iter=2000)\n",
    "    model_p1_a3.fit(train_a3_X1, train_y)\n",
    "\n",
    "    model_p2_a3 = LogisticRegression(max_iter=2000)\n",
    "    model_p2_a3.fit(train_a3_X2, train_y)\n",
    "  \n",
    "    return model_p1_a3, model_p2_a3\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nlp1, nlp2 = setup_pipeline()\n",
    "    train_ds, test_ds = load_data()\n",
    "    train_ds, test_ds, train_y, test_y = prepare_data(train_ds, test_ds, nlp1, nlp2)\n",
    "\n",
    "    train_a1_X1, train_a1_X2, test_a1_X1, test_a1_X2 = embedding_word2vec(train_ds, test_ds)\n",
    "    train_a2_X1, train_a2_X2, test_a2_X1, test_a2_X2 = embedding_bow(train_ds, test_ds)\n",
    "    train_a3_X1, train_a3_X2, test_a3_X1, test_a3_X2 = embedding_glove(train_ds, test_ds)\n",
    "\n",
    "    model_p1_a1, model_p2_a1 = train_model_word2vec(train_a1_X1, train_a1_X2, train_y)\n",
    "    model_p1_a2, model_p2_a2 = train_model_bow(train_a2_X1, train_a2_X2, train_y)\n",
    "    model_p1_a3, model_p2_a3 = train_model_glove(train_a3_X1, train_a3_X2, train_y) \n",
    "\n",
    "    print(f\"Model Pipeline1 Word2Vec Accuracy: {model_p1_a1.score(test_a1_X1, test_y):>10.4f}\")\n",
    "    print(f\"Model Pipeline2 Word2Vec Accuracy: {model_p2_a1.score(test_a1_X2, test_y):>10.4f}\")\n",
    "    print(f\"Model Pipeline1 BOW      Accuracy: {model_p1_a2.score(test_a2_X1, test_y):>10.4f}\")\n",
    "    print(f\"Model Pipeline2 BOW      Accuracy: {model_p2_a2.score(test_a2_X2, test_y):>10.4f}\")\n",
    "    print(f\"Model Pipeline1 GloVe    Accuracy: {model_p1_a3.score(test_a3_X1, test_y):>10.4f}\")   \n",
    "    print(f\"Model Pipeline2 GloVe    Accuracy: {model_p2_a3.score(test_a3_X2, test_y):>10.4f}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
